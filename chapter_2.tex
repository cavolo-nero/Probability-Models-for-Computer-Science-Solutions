\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper]{geometry}
\usepackage{nicefrac}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{Var}
\title{Solutions to Chapter 2 of Sheldon M. Ross' \textit{Probability Models For Computer Science}}
\date{}
\begin{document}
    \maketitle
\begin{enumerate}
\item
    Let $q=1-p$.
    \begin{enumerate}
    \item
        The vertex set $\left\{1,...,k\right\}$ is a component of the graph if it is connected and
        there are no edges between nodes from the respective sets $\left\{1,...,k\right\}$ and $\left\{k+1,...,n\right\}$.
        \[ P_k \cdot \left(q^{n-k}\right)^k \]

    \item
        The probability that vertex 1 is a member of a component of size $k$ is the sum of the probabilities of all
        subsets of size $k$ containing vertex 1 is a component.
        \[ {n-1 \choose k-1} \cdot P_k \cdot q^{k(n-k)} \]

    \item
        The graph of $n$ nodes is connected if an arbitrary vertex, say vertex 1, belongs to a component of size $n$; that is,
        the vertex does not belong to a component of size other than $n$.
        \[ P_n = 1 - \sum_{k=1}^{n-1} {n-1 \choose k-1} \cdot P_k \cdot q^{k(n-k)} \]
    \item
        \[\begin{split}
            P_1     & = 1 \\
            P_2     & = 1 - q \\
            P_3     & = 1 - 3q^2 + 2q^3 \\
            P_4     & = 1 - 4q^3 - 3q^4 + 12q^5 - 6q^6 \\
            P_5     & = 1 - 5q^4 - 10q^6 + 20q^7 + 30q^8 - 60q^9 + 24q^{10} \\
            P_6     & = 1 - 6q^5 - 15q^8 + 20q^9 + 120q^{11} - 90q^{12} - 270q^{13} + 360q^{14} - 120q^{15}
        \end{split}\]
    \end{enumerate}
\item
    Upon choosing a pivot for a sequence of $n>0$ distinct elements, $n-1$ comparisons are made before recursively sorting two subsets.
    If the pivot element is the $i$th rank, the sizes of the two subsets are $i-1$ and $n-i$.

    Hence, for $n>0$, noting also that the pivot is selected uniformly at random among $n$ elements,
    \[\begin{split}
    M_n & = (n-1) + \sum_{i=1}^n \frac{1}{n} \left( M_{i-1} + M_{n-i} \right ) \\
        & = n - 1 + \frac{1}{n} \left( \sum_{i=1}^n M_{i-1} + \sum_{i=1}^n M_{n-i} \right) \\
        & = n - 1 + \frac{2}{n} \sum_{j=0}^{n-1} M_j \text.
    \end{split}\]

    Using base case $M_0=0$,
    \begin{center}
    $\begin{array}{c|cccccccc}
        i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
        \hline
        M_i & 0 & 1 & \nicefrac{8}{3} & \nicefrac{29}{6} & \nicefrac{37}{5} & \nicefrac{103}{10} & \nicefrac{472}{35} & \nicefrac{2369}{140} \\
    \end{array}$.
    \end{center}
\item
    Let $C$ denote the number of comparisons and
    let $R$ denote the number of remaining values in the non-empty pile when the other is first emptied.
    Suppose $n$ is positive.

    Every comparison corresponds to the removal of one value whilst both piles are non-empty.
    Given there are $n$ values in total, and $R$ remaining values upon the first emptying of a pile,
    there must have been $n-R$ comparisons.

    It is clear that the remaining values is some maximal suffix of the sorted sequence.
    Hence, $R$ is the number of sequential values, in descending order from the largest value, that were allocated to the same pile.
    The largest value always remains in the non-empty pile, hence $R \geq 1$, and, noting that all values were equally
    likely to be have been initially placed in either pile,
    \[ P(R \geq i) = \left(\frac{1}{2}\right)^{i-1} \]
    for $1 \leq i \leq n$.

    Using $E[X] = \sum_{i=1}^\infty P(X \geq i)$ for a non-negative random variable $X$,
    \[\begin{split}
    E[C]    & = E[n-R] \\
            & = n - \sum_{i=1}^\infty P(R \geq i) \\
            & = n - \sum_{i=1}^n \left(\frac{1}{2}\right)^{i-1} \\
            % & = n - \sum_{i=0}^{n-1} \left(\frac{1}{2}\right)^i \\
            & = n - 2 + \left(\frac{1}{2}\right)^{n-1} \text.
    \end{split}\]
\item
    Define $Q_{i,j} = 1 - P_i - P_j$, the probability that neither $e_i$ nor $e_j$ are requested at a particular time.

    Let $I_{i,j}$ indicate the event that element $e_i$ precedes $e_j$ at time $n$.
    This event can occur if
    \begin{itemize}
    \item  $e_i$ initially preceded $e_j$ and neither $e_i$ nor $e_j$ were selected,
    which happens with probability $\frac{1}{2} \cdot Q_{i,j}^{n-1}$ since each element is equally likely
    to have preceded the other in the initial ordering; or
    \item $e_j$ is not selected after the last time $e_i$ is selected.
    Conditioning on the time of the last selection of $e_i$, this occurs with probability $\sum_{k=1}^{n-1} P_i \cdot Q_{i,j}^{n-k-1}$.
    \end{itemize}

    Let $R_i$ denote the position of $e_i$ at time $n$.
    \[ R_i = 1 + \sum_{j\neq i}I_{j,i} \]

    Let $R$ denote the position of the element requested at time $n$.
    Conditioning on the element requested at time $n$, its expected value is
    \[\begin{split}
    E[R]    & = \sum_i P_i E[R_i] \\
            & = \sum_i P_i \left( 1 + \sum_{j \neq i}E[I_{j,i}] \right) \\
            & = \sum_i P_i + \sum_i P_i \sum_{j \neq i} \left( \frac{1}{2} \cdot Q_{i,j}^{n-1} + \sum_{k=1}^{n-1} P_j \cdot Q_{i,j}^{n-k-1} \right) \\
            & = 1 + \sum_i \sum_{j \neq i} \frac{1}{2} \cdot P_i \cdot Q_{i,j}^{n-1} + \sum_i \sum_{j \neq i} \sum_{k=1}^{n-1} P_i \cdot P_j \cdot Q_{i,j}^{n-k-1} \text.
    \end{split}\]
\item
    Let $p_{i,j}$ denote the probability that $e_i$ precedes $e_j$, and $R$ the position of the requested element.
    Given the requested element, its position is $1$ more the number of elements that precede it.
    Hence, conditioning on the requested element, the expected position of the requested element is
    \[\begin{split}
    E[R]    & = 1 + \sum_iP_i \sum_{j\neq i} p_{j,i} \\
            & = 1 + \sum_i\sum_{j<i} \left( p_{j,i}P_i + p_{i,j}P_j \right) \\
            & = 1 + \sum_i\sum_{j<i} \left( (1-p_{i,j})P_i + p_{i,j}P_j \right) \\
            & = 1 + \sum_i\sum_{j<i} \left( P_i + p_{i,j}( P_j-P_i ) \right) \text. \\
            % & = 1 + \sum_i\sum_{j<i} P_i + \sum_i\sum_{j<i} p_{i,j}(P_j-P_i) \text. \\
    \end{split}\]
    The expected position can be minimised by $p_{i,j}=1$ where $P_i\geq P_j$; that is, ordering the elements in
    decreasing order of their probabilities of being selected.
\item
    Let $F$ denote the number of fixed points
    and, for $1 \leq i \leq n$, let $I_i$ indicate that position $i$ is a fixed point of the random permutation.
    Then,
    \[ F = I_1 + ... + I_n \text. \]
    The expected value of the Bernoulli random variable $I_i$ is $\frac{1}{n}$, so the expected number of fixed points is
    \[ E[F] = \sum_{i=1}^nE[I_i] = n\cdot\frac{1}{n} = 1 \text. \]

    Let $p_{i,j}$ denote the joint probability mass function of $I_i$ and $I_j$, for $i\neq j$.
    It is clear that the function is symmetric; $p_{i,j}(a,b)$ = $p_{i,j}(b,a)$.

    The covariance of $I_i$ and $I_j$, for $i\neq j$, is
    \[\begin{split}
    \Cov(I_i, I_j)  & = E[(I_i-E[I_i])(I_j-E[I_j])] \\
                    & = \sum_{x=0}^1\sum_{y=0}^1 p_{i,j}(x,y)\cdot \left(x-\frac{1}{n}\right)\left(y-\frac{1}{n}\right) \\
                    & = p_{i,j}(0,0) \cdot \frac{1}{n^2} + 2p_{i,j}(1,0) \cdot \frac{1-n}{n^2} + p_{i,j}(1,1) \cdot \frac{(n-1)^2}{n^2} \\
                    & = \left(\frac{1}{n} + \frac{n-2}{n}\frac{n-2}{n-1}\right)\cdot\frac{1}{n^2}
                        + 2\left(\frac{1}{n}\frac{n-2}{n-1}\right)\cdot\frac{1-n}{n^2}
                        + \left(\frac{1}{n}\frac{1}{n-1}\right)\cdot\frac{(n-1)^2}{n^2} \\
                    & = \frac{n^2-3n+3}{n^3(n-1)} + \frac{-2n^2+6n-4}{n^3(n-1)} + \frac{n^2-2n+1}{n^3(n-1)} \\
                    & = \frac{1}{n^2(n-1)} \text.
    \end{split}\]

    Hence, given also that the variance of the Bernoulli random variable $I_i$ is $\frac{1}{n}\left(1-\frac{1}{n}\right)=\frac{n-1}{n^2}$, the variance of the number of fixed points is
    \[\begin{split}
    \Var(F) & = \sum_{i=1}^n\Var(I_i) + \sum_i\sum_{j\neq i}\Cov(I_i,I_j) \\
            & = n\cdot\frac{n-1}{n^2} + n(n-1)\cdot \frac{1}{n^2(n-1)} \\
            & = 1 - \frac{1}{n} + \frac{1}{n} \\
            & = 1 \text.
    \end{split}\]

    An alternative method of calculating the variance computes the expected value of $F^2$.
    \[\begin{split}
    E[F^2]  & = E\left[\left(\sum_{i=1}^nI_i\right)^2\right] \\
            & = E\left[\sum_{i=1}^nI_i^2\right] + E\left[\sum_{i=1}^n\sum_{j\neq i}^nI_iI_j\right] \\
            & = \sum_{i=1}^n E[I_i^2] + \sum_{i=1}^n\sum_{j\neq i}^n E[I_iI_j] \\
            & = \sum_{i=1}^n E[I_i] + \sum_{i=1}^n\sum_{j\neq i}^n p_{i,j}(1,1) \\
            & = n \cdot \frac{1}{n} + n(n-1) \cdot \left(\frac{1}{n}\frac{1}{n-1}\right) \\
            & = 2
    \end{split}\]
    Then the variance is $\Var(F) = E[F^2] - E[F]^2 = 2 - 1^2 = 1$.
\item
    Let $j_1, ..., j_n$ be an arbitrary permutation of $1, ..., n$.

    Supposing $X$ is a random permutation, the probability of
    \begin{equation}\label{eq:randomperm}X(j_1)=1, ..., X(j_n)=n \end{equation}
    is $\nicefrac{1}{n!}$.
    Equation \eqref{eq:randomperm} may be restated
    \[ j_1 = X^{-1}(1), ..., j_n = X^{-1}(n) \]
    so
    \[ P\left\{( X^{-1}(1), ..., X^{-1}(n) ) = (j_1, ..., j_n)\right\} = \frac{1}{n!} \text. \]
\item
    Let $P$ denote the number of matched pairs, and let $I_i$ for $0 < i \leq n$ indicate that $i$ belongs to a matched pair.
    % Let $p_{i,j}$ denote the joint probability distribution of $I_i$ and $I_j$ for $i\neq j$.

    \[ P = \frac{I_1 + ... + I_n}{2} \]

    \begin{enumerate}
    \item
        If there is a single person, $n=1$, there are no matched pairs, $P=0$.

        Otherwise, supposing $n>1$, the expected value of $I_i$ is the probability that $i$ belongs to a matched pair,
        \[ E[I_i] = \frac{n-1}{n}\frac{1}{n-1} = \frac{1}{n} \text, \]
        and the expected value of $P$ is
        \[ \frac{1}{2}\sum_{i=1}^n I_i = \frac{1}{2}\left( n \cdot \frac{1}{n} \right) = \frac{1}{2} \text.  \]

        Hence,
        \[ E[P] =
            \begin{cases}
                0 & \text{if } n=1 \\
                \frac{1}{2} & \text{otherwise}
            \end{cases} \text.
        \]
    \item
        For $i \neq j$, the probability $i$ and $j$ belong to the same matched pair (i.e., $i$ chooses $j$'s hat and $j$ chooses $i$'s hat) is
        \[ P(I_i=1,I_j=1,i \text{ and } j \text{ matched pair}) = \frac{1}{n}\frac{1}{n-1} \text. \]

        The probability that both $i$ and $j$ belong to matched pairs, but not the same, depends on $n$. If $n<4$,
        it is not possible for $i$ and $j$ to belong to distinct pairs. Otherwise, if $n \geq 4$, it
        is the probability that $i$ matches with some other than $j$, with probability $\frac{n-2}{n}\frac{1}{n-1}$,
        and $j$ belongs to a matched pair amongst the remaining, with probability $\frac{n-3}{n-2}\frac{1}{n-3}$.
        Putting that together,
        \[ P(I_i=1,I_j=1,i \text{ and } j \text{ not matched pair}) =
                \begin{cases}
                    0 & \text{if } 1 < n < 4 \\
                    \frac{1}{n(n-1)} & \text{otherwise}
                \end{cases} \text.
        \]

        Hence, for $i \neq j$,
        \[ P(I_i=1,I_j=1) =
                \begin{cases}
                    \frac{1}{n(n-1)} & \text{if } 1<n<4 \\
                    \frac{2}{n(n-1)} & \text{otherwise}
                \end{cases} \text.
        \]

        If there is a single person, $n=1$, there is $P$ has zero variance since $P=0$.

        Otherwise, supposing $n>1$, the variance of $P$ is
        \[\begin{split}
        E[P^2] - E[P]^2 & = E\left[\left(\frac{I_1 + ... + I_n}{2}\right)^2\right] - E[P]^2 \\
                        & = \frac{1}{4}\left( \sum_{i=1}^n E[I_i^2] + \sum_i\sum_{j \neq i}E[I_iI_j] \right) - \left(\frac{1}{2}\right)^2 \\
                        & = \frac{1}{4}\left( n\cdot\frac{1}{n} + \sum_i\sum_{j \neq i}P(I_i=1,I_j=1) \right) - \frac{1}{4} \\
                        & = \frac{1}{4}\sum_i\sum_{j \neq i}P(I_i=1,I_j=1) \text.
        \end{split}\]

        Hence,
        \[ \Var(P) =
                \begin{cases}
                    0 & \text{if } n=1 \\
                    \frac{1}{4} & \text{if } 1 < n < 4 \\
                    \frac{1}{2} & \text{otherwise}
                \end{cases} \text.
        \]
    \item
        Let $C$ denote the length of the cycle containing an arbitrarily chosen value, say the first.
        The predicate there are no pairs is equivalent to the predicate there are no cycles of length 2. Then,
        defining $S_n = \{ x \in \mathbb{Z} : x \neq 2, 1 \leq x \leq n \}$, for $n>0$
        \[\begin{split}
        Q_n & = \sum_{i \in S_n} P(C=i)\cdot Q_{n-i} \\
            & = \frac{1}{n} \sum_{i \in S_n} Q_{n-i} \text. \\
        \end{split}\]

    Using base case $Q_0=0$,
    \begin{center}
    $\begin{array}{c|cccccccc}
        i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
        \hline
        Q_i & 1 & \nicefrac{1}{2} & \nicefrac{1}{2} & \nicefrac{5}{8} & \nicefrac{5}{8} & \nicefrac{29}{48} & \nicefrac{29}{48} & \nicefrac{233}{384}
    \end{array}$.
    \end{center}
    \end{enumerate}
\item
    Let $C_1$ denote the size of the cycle that contains $1$.
    \begin{enumerate}
    \item
        With base case $M_0=0$,
        \[\begin{split}\label{eq:recurrenceM}
        M_n & = E[N] \\
            & = \sum_{i=1}^n E[N|C_1=i]\cdot P(C_1=i) \\
            & = \sum_{i=1}^n (1+M_{n-i})\cdot \frac{1}{n} \\
            & = 1 + \frac{1}{n}\sum_{j=0}^{n-1} M_j \text.
        \end{split}\]
    \item
        % TODO awful argument
        There are $i$ values in a cycle of size $i$, and $i\cdot\frac{1}{i}=1$, so the term in the summation corresponding
        to a given value contributes that value's part in its cycle.

        \[\begin{split}
        E[N]    & = \sum_{j=1}^n E\left[\frac{1}{C_j}\right] \\
                & = \sum_{j=1}^n \sum_{i=1}^n P( C_j=i ) \cdot \frac{1}{i} \\
                & = \sum_{j=1}^n \sum_{i=1}^n \frac{1}{n} \cdot \frac{1}{i} \\
                & = \sum_{i=1}^n \frac{1}{i} \\
        \end{split}\]

        This result can be confirmed by induction to be the solution to the recurrence relation \eqref{eq:recurrenceM}.

        When $n=0$,
        \[ M_0 = \sum_{j=0}^{-1} \frac{1}{j} = 0 \]
        as required.

        Suppose $M_i = \sum_{j=1}^i \frac{1}{j}$ for $0 \leq i \leq k$ and arbitrary integer $k$.
        The below shows that $M_{k+1}=\sum_{i=1}^{k+1} \frac{1}{i}$.
        \[\begin{split}
            M_{k+1} & = 1 + \frac{1}{k+1}\sum_{i=0}^{k} M_i \\
                    & = 1 + \frac{1}{k+1}\sum_{i=1}^{k} \sum_{j=1}^i \frac{1}{j} \\
                    & = 1 + \frac{1}{k+1}\sum_{j=1}^{k} \sum_{i=j}^k \frac{1}{j} \\
                    & = 1 + \frac{1}{k+1}\sum_{j=1}^{k} \frac{k-j+1}{j} \\
                    & = 1 + \frac{1}{k+1}\sum_{j=1}^{k} \left(-1 + \frac{k+1}{j}\right) \\
                    & = 1 - \frac{k}{k+1} + \sum_{j=1}^k\frac{1}{j} \\
                    & = \sum_{j=1}^{k+1}\frac{1}{j} \\
        \end{split}\]
    \item
        Let $D$ denote the event that $1, 2, ..., k$ are all in the same cycle, supposing $1\leq k\leq n$.

        If $D$ occurs, the size of the cycle containing $1$ must be at least $k$.
        Hence, conditioning on $C_1$ and using the relationship
        \[ {i \choose j-1} + {i \choose j} = {i+1 \choose j} \text, \]
        \[\begin{split}
        P(D)    & = \sum_{x=k}^n P(C_1=x) P(D|C_1=x) \\
                & = \sum_{x=k}^n \frac{1}{n} \left( \frac{x-1}{n-1}\cdot...\cdot\frac{x-(k-1)}{n-(k-1)} \right) \\
                & = \frac{(n-k)!}{n!} \sum_{x=k}^n \frac{(x-1)!}{(x-k)!} \\
                & = \frac{(n-k)!}{n!} \sum_{x=k}^n {x-1 \choose x-k}(k-1)! \\
                & = \frac{k!}{k} \frac{(n-k)!}{n!} \sum_{y=0}^{n-k} {y+k-1 \choose y} \\
                & = \frac{1}{k} {n \choose k}^{-1} \left[ \sum_{y=0}^{n-k} {y+k-1 \choose y} \right] \\
                & = \frac{1}{k} {n \choose k}^{-1} \left[ {0+k-1 \choose 0} + \sum_{y=1}^{n-k} {y+k-1 \choose y} \right] \\
                & = \frac{1}{k} {n \choose k}^{-1} \left[ 1 + \sum_{y=1}^{n-k} \left( {y+k \choose y} - {y-1+k \choose y-1} \right) \right] \\
                & = \frac{1}{k} {n \choose k}^{-1} \left[ 1 + \sum_{y=1}^{n-k} {y+k \choose y} - \sum_{y=0}^{n-k-1}{y+k \choose y} \right] \\
                & = \frac{1}{k} {n \choose k}^{-1} \left[ 1 + {n-k+k \choose n-k} - {0+k \choose 0} \right] \\
                & = \frac{1}{k} {n \choose k}^{-1} \left[ 1 + {n \choose k} - 1 \right] \\
                & = \frac{1}{k}.
        \end{split}\]
    \item
        Let $E$ denote the event that $1,2,...,k$ is a cycle, supposing $1 \leq k \leq n$.

        The probability of this event is the probability $1$ is in a cycle of size $k$, $P(C_1=k)=\frac{1}{n}$;
        and the other values in that cycle are $2,...,k$, which denotes one of the ${n-1 \choose k-1}$ possible cycles of size $k$.

        Putting this together,
        \[\begin{split}
        P(E)    & = \frac{1}{n}{n-1 \choose k-1}^{-1} \\
                & = \frac{(k-1)!(n-k)!}{n(n-1)!} \\
                & = \frac{k!}{k} \frac{(n-k)!}{n!} \\
                & = \frac{1}{k}{n \choose k}^{-1} \text.\\
        \end{split}\]
    \end{enumerate}
\item
    Let $M$ denote the number of maximal increasing subsequences in a random permutation of $n$ values.
    Let $I_j$ indicate that $j$ is the beginning of a maximal subsequence for $1 \leq j \leq n$.

    The probability that $j$ begins a maximal increasing subsequence is independent of whether
    $k$, for $k>j$, also begins a maximal increasing subsequence.

    Hence, using the independence of $I_j$ and $I_k$ for $j \neq k$,
    \[\begin{split}
    \Var(M) & = \Var\left(\sum_j{I_j}\right) \\
            & = \sum_j{\Var(I_j)} \\
            & = \sum_j\left(E(I_j^2) - E(I_j)^2\right) \\
            & = \sum_j{\left(\frac{1}{j} - \left(\frac{1}{j}\right)^2\right)} \\
            & = \sum_j{\frac{j-1}{j^2}}\text.
    \end{split}\]
\item
    Let $I_i$, for $1 \leq i < n$ indicate that $X(i) < X(i+1)$ for a random permutation $X$.
    Then, the number of rises in a random permutation of $n > 0$ values is
    \[ N = \sum_{i=1}^{n-1}{I_i}\text. \]
    \begin{enumerate}
    \item
        \[\begin{split}
        E[N]    & = \sum_{i=1}^{n-1}{E[I_i]} \\
                & = \sum_{i=1}^{n-1}{P(X(i) < X(i+1))} \\
                & = \sum_{i=1}^{n-1}\frac{1}{2} \\
                & = \frac{n-1}{2}
        \end{split}\]
    \item
        It is clear that a random permutation of $n < 2$ elements has no variance in the number of rises
        since there are none. Consider instead the number of the rises in a random permutation of $n \geq 2$ elements.

        A random permutation with consecutive rises at a given position has three numbers in increasing order,
        of which there are ${n \choose 3}$ choices, with the remaining $n-3$ elements occurring in any order.
        Hence, counting the number of permutations with consecutive rises at a given position, for $i < n-1$,
        \[P(I_i=1, I_{i+1}=1) = \frac{{n \choose 3} (n-3)!}{n!} = \frac{1}{6}\text.\]

        A random permutation with rises at two given non-adjacent positions has a pair of increasing numbers at
        the first position, of which there are ${n \choose 2}$ choices, with another pair at the second position,
        of which there are ${n-2 \choose 2}$ choices, and the remaining $n-4$ numbers occurring in any order.
        Hence, counting the number of permutations with rises at two given non-adjacent positions,
        for $j > i + 1$,
        \[P(I_i=1, I_j=1) = \frac{{n \choose 2}{n-2 \choose 2}(n-4)!}{n!} = \frac{1}{4}\text. \]

        Hence, for $n \geq 2$,
        \[\begin{split}
        E[N^2]  & = E\left[\left(\sum_{i=1}^{n-1}I_i\right)^2\right] \\
                & = E\left[\sum_{i=1}^{n-1} I_i^2\ + 2\sum_{i=1}^{n-1}\sum_{j>i}^{n-1}I_iI_j\right] \\
                & = \sum_{i=1}^{n-1}P(I_i=1) + 2\sum_{i=1}^{n-1}\sum_{j>i}^{n-1}P(I_i=1, I_j=1) \\
                & = \frac{n-1}{2} + 2\sum_{i=1}^{n-2}\sum_{j=i+1}^{n-1}P(I_i=1, I_j=1) \\
                & = \frac{n-1}{2} + 2\sum_{i=1}^{n-2}P(I_i = 1, I_{i+1} = 1) + 2\sum_{i=1}^{n-2}\sum_{j=i+2}^{n-1}P(I_i = 1, I_j = 1) \\
                & = \frac{n-1}{2} + 2(n-2) \cdot \frac{1}{6} + (n-2)(n-3) \cdot \frac{1}{4}
        \end{split}\]
        so
        \[\begin{split}
        \Var(N) & = E[N^2] - E[N]^2 \\
                & = \frac{n-1}{2} + \frac{n-2}{3} + \frac{(n-2)(n-3)}{4} - \left(\frac{n-1}{2}\right)^2 \\
                & = \frac{n+1}{12}\text.
        \end{split}\]
    \end{enumerate}
\item
    Letting
    \[ I_{i,j} =
        \begin{cases}
            1 & \text{if } p_i < p_j \\
            0 & \text{otherwise}
        \end{cases} \text,
    \]
    define the position of $i$ in the weighted random permutation $p_i = 1 + \sum_{j \neq i}I_{j,i} $.

    \begin{enumerate}
    \item
    \[\begin{split}
    E[p_i]  & = 1 + \sum_{j \neq i}E[I_{j,i}] \\
            & = 1 + \sum_{j \neq i}P(I_{j,i}=1) \\
            & = 1 + \sum_{j \neq i}\frac{\lambda_j}{\lambda_i + \lambda_j} \text.
    \end{split}\]
    \item
    Given, for $i \neq j,k$ and $j \neq k$,
    \[\begin{split}
    P(I_{j,i}=1,I_{k,i}=1)  & = P(I_{j,i}=1, I_{k,i}=1 , I_{j,k} = 1) + P(I_{j,i}=1, I_{k,i}=1 , I_{j,k} = 0) \\
                            & = \frac{\lambda_j}{\lambda_i+\lambda_j+\lambda_k}\cdot\frac{\lambda_k}{\lambda_i+\lambda_k}+\frac{\lambda_k}{\lambda_i+\lambda_j+\lambda_k}\cdot\frac{\lambda_j}{\lambda_i+\lambda_j} \\
                            & =\frac{\lambda_j\lambda_k}{\lambda_i+\lambda_j+\lambda_k}\left(\frac1{\lambda_i+\lambda_k}+\frac1{\lambda_i+\lambda_j}\right)\text,
    \end{split}\]

    we have
    \begin{eqnarray*}
    &&
    \sum_{j,k\ne i\atop k\ne j}\left(E\left[I_{j,i}I_{k,i}\right]-E\left[I_{j,i}\right]E\left[I_{k,i}\right]\right)
    \\
    &=&
    \sum_{j,k\ne i\atop k\ne j}\left(\frac{\lambda_j\lambda_k}{\lambda_i+\lambda_j+\lambda_k}\left(\frac1{\lambda_i+\lambda_k}+\frac1{\lambda_i+\lambda_j}\right)-\frac{\lambda_j}{\lambda_i+\lambda_j}\cdot\frac{\lambda_k}{\lambda_i+\lambda_k}\right)
    \\
    &=&
    \sum_{j,k\ne i\atop k\ne j}\left(\frac{\lambda_j\lambda_k}{(\lambda_i+\lambda_j)(\lambda_i+\lambda_k)}\left(\frac{2\lambda_i+\lambda_j+\lambda_k}{\lambda_i+\lambda_j+\lambda_k}-1\right)\right)
    \\
    &=&
    \lambda_i\sum_{j,k\ne i\atop k\ne j}\frac{\lambda_j\lambda_k}{(\lambda_i+\lambda_j)(\lambda_i+\lambda_k)(\lambda_i+\lambda_j+\lambda_k)}
    \end{eqnarray*}

    and, together with
    \begin{eqnarray*}
    \sum_{j\ne i}\left(E\left[I_{j,i}^2\right]-E\left[I_{j,i}\right]^2\right)
    &=&
    \sum_{j\ne i}\left(E\left[I_{j,i}\right]-E\left[I_{j,i}\right]^2\right)
    \\
    &=&
    \sum_{j\ne i}\left(E\left[I_{j,i}\right]\left(1-E\left[I_{j,i}\right]\right)\right)
    \\
    &=&
    \sum_{j\ne i}\frac{\lambda_j}{\lambda_i+\lambda_j}\cdot\frac{\lambda_i}{\lambda_i+\lambda_j}
    \\
    &=&
    \lambda_i\sum_{j\ne i}\frac{\lambda_j}{\left(\lambda_i+\lambda_j\right)^2}\text,
    \end{eqnarray*}

    we have
    \begin{eqnarray*}
    \Var(p_i)
    &=&
    \Var(p_i-1)
    \\
    &=&
    \Var\left(\sum_{j\ne i}I_{j,i}\right)
    \\
    &=&
    E\left[\left(\sum_{j\ne i}I_{j,i}\right)^2\right]- E\left[\sum_{j\ne i}I_{j,i}\right]^2
    \\
    &=&
    \sum_{j,k\ne i\atop k\ne j}\left(E\left[I_{j,i}I_{k,i}\right]- E\left[I_{j,i}\right] E\left[I_{k,i}\right]\right)+\sum_{j\ne i}\left(E\left[I_{j,i}^2\right]-E\left[I_{j,i}\right]^2\right)
    \\
    &=&
    \lambda_i\sum_{j\ne i}\frac{\lambda_j}{\lambda_i+\lambda_j}\left(\frac1{\lambda_i+\lambda_j}+\sum_{k\ne i,j}\frac{\lambda_k}{(\lambda_i+\lambda_k)(\lambda_i+\lambda_j+\lambda_k)}\right)\cite{4846976}\text.
    \end{eqnarray*}
    \end{enumerate}
\end{enumerate}

\bibliography{chapter_2.bib}{}
\bibliographystyle{plain}
\end{document}
